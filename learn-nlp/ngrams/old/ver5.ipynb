{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "with open('testing_data.txt', 'r') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "with open('training_data.txt', 'r') as f:\n",
    "    train_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram: \n",
    "    def __init__(self):\n",
    "        self.n_unigram = 0\n",
    "        self.vocab_size = 0 \n",
    "        self.count = {} \n",
    "        # self.prob = {} \n",
    "    \n",
    "    # def prob_with_smoothing(self): \n",
    "    #     proba = {}\n",
    "    #     M = self.n_unigram + self.vocab_size \n",
    "\n",
    "    #     for unigram in self.count.keys(): \n",
    "    #         if unigram == \"<UNK>\":\n",
    "    #             proba[\"<UNK>\"] = 1 / M\n",
    "    #         else: \n",
    "    #             proba[unigram] = (self.count[unigram] + 1) / M \n",
    "\n",
    "    #     return proba \n",
    "\n",
    "    def train(self, train_data): \n",
    "        unigrams = nltk.tokenize.word_tokenize(train_data)\n",
    "        self.count[\"<UNK>\"] = 0\n",
    "        self.n_unigram = len(unigrams)\n",
    "        self.vocab_size = len(set(unigrams)) \n",
    "\n",
    "        # creating unigram count dict \n",
    "        for unigram in unigrams:\n",
    "            if unigram in self.count.keys(): \n",
    "                self.count[unigram] += 1 \n",
    "            else: \n",
    "                self.count[unigram] = 1 \n",
    "        \n",
    "        # self.prob = self.prob_with_smoothing() \n",
    "\n",
    "    def compute_prob(self, unigram):\n",
    "        N = self.n_unigram\n",
    "        V = self.vocab_size\n",
    "        if (unigram in self.count.keys()):\n",
    "            return (self.count[unigram] + 1) / (N + V)\n",
    "        else: \n",
    "            return 1 / (N + V)\n",
    "    \n",
    "    def test_perplexity(self, test_data): \n",
    "        test_unigrams = nltk.tokenize.word_tokenize(test_data)\n",
    "        N = len(test_unigrams) \n",
    "\n",
    "        probs = [] \n",
    "        for unigram in test_unigrams: \n",
    "            probs.append(self.compute_prob(unigram))\n",
    "        \n",
    "        avg_log_likelihood = np.log(probs).sum() / N\n",
    "        ppl = np.exp((-1) * avg_log_likelihood)\n",
    "        return ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172.11286794438394"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = Unigram() \n",
    "unigram_model.train(train_data)\n",
    "\n",
    "unigram_test_ppl = unigram_model.test_perplexity(test_data)\n",
    "unigram_test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams:  742\n",
      "Number of unique unigrams:  302\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unigrams: \", unigram_model.n_unigram) \n",
    "print(\"Number of unique unigrams: \", len(unigram_model.count)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram: \n",
    "    def __init__(self):\n",
    "        self.count_unigram = {}\n",
    "        self.count_bigram = {} \n",
    "        self.n_unigram = 0\n",
    "        self.n_bigram = 0 \n",
    "\n",
    "        self.vocab_size = 0 \n",
    "        # self.prob_bigram = {} \n",
    "    \n",
    "    # def prob_with_smoothing(self): \n",
    "    #     proba = {} \n",
    "    #     V = self.vocab_size\n",
    "    #     proba[\"<UNK>\"] = 1/ (self.n_bigram + V)\n",
    "\n",
    "    #     for bigram in self.count_bigram.keys(): \n",
    "    #         if bigram != \"<UNK>\": \n",
    "    #             ctx = bigram[0]\n",
    "    #             count_context = self.count_unigram[ctx]\n",
    "    #             count_joint = self.count_bigram[bigram]\n",
    "    #             proba[bigram] = (count_joint + 1) / (count_context + V)\n",
    "\n",
    "    #     return proba \n",
    "    \n",
    "    def get_ngram(self, n, text): \n",
    "        unigrams = nltk.tokenize.word_tokenize(text)\n",
    "        ngrams = [] \n",
    "        if n == 1: \n",
    "            return unigrams \n",
    "        else: \n",
    "            last_start = len(unigrams) - n + 1\n",
    "            for i in range(last_start):\n",
    "                ngram = tuple(unigrams[i: i+n])\n",
    "                ngrams.append(ngram)\n",
    "            return ngrams\n",
    "    \n",
    "    def count_ngram(self, ngrams): \n",
    "        count = {} \n",
    "        count[\"<UNK>\"] = 0\n",
    "        \n",
    "        for ngram in ngrams: \n",
    "            if ngram in count.keys(): \n",
    "                count[ngram] += 1 \n",
    "            else: \n",
    "                count[ngram] = 1 \n",
    "        return count \n",
    "\n",
    "    def train(self, train_data):\n",
    "        unigrams = self.get_ngram(1, train_data)\n",
    "        self.n_unigram = len(unigrams)\n",
    "        self.vocab_size = len(set(unigrams))\n",
    "        self.count_unigram = self.count_ngram(unigrams)\n",
    "\n",
    "        bigrams = self.get_ngram(2, train_data)\n",
    "        self.n_bigram = len(bigrams) \n",
    "        self.count_bigram = self.count_ngram(bigrams)\n",
    "\n",
    "        # self.prob_bigram = self.prob_with_smoothing() \n",
    "\n",
    "    def compute_prob(self, bigram):\n",
    "        '''bigram, self.count_unigram, self.count_bigram'''\n",
    "        ctx = bigram[0]\n",
    "        if (ctx in self.count_unigram.keys()):\n",
    "            context = self.count_unigram[ctx]\n",
    "        else:\n",
    "            context = 0\n",
    "\n",
    "        if (bigram in self.count_bigram.keys()):\n",
    "            joint = self.count_bigram[bigram]\n",
    "        else: \n",
    "            joint = 0 \n",
    "        \n",
    "        return (joint + 1) / (context + self.vocab_size)\n",
    "\n",
    "    def test_perplexity(self, test_data): \n",
    "        test_bigrams = self.get_ngram(2, test_data)\n",
    "        test_unigrams = self.get_ngram(1, test_data)\n",
    "        N = len(test_unigrams) \n",
    "\n",
    "        probs = [] \n",
    "        \n",
    "        first_word = test_unigrams[0] \n",
    "        p_first_word = 1\n",
    "        if (first_word in self.count_unigram.keys()):\n",
    "            p_first_word = (self.count_unigram[first_word] + 1) / (self.n_unigram + self.vocab_size)\n",
    "        else: \n",
    "            p_first_word = 1 / (self.n_unigram + self.vocab_size)\n",
    "\n",
    "        probs.append(p_first_word)\n",
    "\n",
    "        for bigram in test_bigrams: \n",
    "            probs.append(self.compute_prob(bigram))\n",
    "        \n",
    "        avg_log_likelihood = np.log(probs).sum() / N\n",
    "        ppl = np.exp((-1) * avg_log_likelihood)\n",
    "        return ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209.69428094428645"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = Bigram()\n",
    "bigram_model.train(train_data)\n",
    "\n",
    "bigram_test_ppl = bigram_model.test_perplexity(test_data)\n",
    "bigram_test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams:  741\n",
      "Number of unique bigrams:  591\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of bigrams: \", bigram_model.n_bigram) \n",
    "print(\"Number of unique bigrams: \", len(bigram_model.count_bigram)-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram: \n",
    "    def __init__(self):\n",
    "        self.count_unigram = {} \n",
    "        self.count_bigram = {} \n",
    "        self.count_trigram = {} \n",
    "        self.n_trigram = 0\n",
    "\n",
    "        self.vocab_size = 0 \n",
    "        self.prob_trigram = {} \n",
    "\n",
    "    def prob_with_smoothing(self): \n",
    "        proba = {} \n",
    "        V = self.vocab_size \n",
    "        proba[\"<UNK>\"] = 1/ (self.n_trigram + V)\n",
    "\n",
    "        for trigram in self.count_trigram.keys(): \n",
    "            if trigram != \"<UNK>\": \n",
    "                ctx = (trigram[0], trigram[1])\n",
    "                count_context = self.count_bigram[ctx]\n",
    "                count_joint = self.count_trigram[trigram]\n",
    "                proba[trigram] = (count_joint + 1) / (count_context + V)\n",
    "        \n",
    "        return proba \n",
    "    \n",
    "    def get_ngram(self, n, text): \n",
    "        unigrams = nltk.tokenize.word_tokenize(text)\n",
    "        ngrams = [] \n",
    "        if n == 1: \n",
    "            return unigrams \n",
    "        else: \n",
    "            last_start = len(unigrams) - n + 1\n",
    "            for i in range(last_start):\n",
    "                ngram = tuple(unigrams[i: i+n])\n",
    "                ngrams.append(ngram)\n",
    "            return ngrams\n",
    "    \n",
    "    def count_ngram(self, ngrams): \n",
    "        count = {} \n",
    "        count[\"<UNK>\"] = 0\n",
    "        \n",
    "        for ngram in ngrams: \n",
    "            if ngram in count.keys(): \n",
    "                count[ngram] += 1 \n",
    "            else: \n",
    "                count[ngram] = 1 \n",
    "        return count \n",
    "\n",
    "    def train(self, train_data):\n",
    "        unigrams = self.get_ngram(1, train_data)\n",
    "        self.vocab_size = len(set(unigrams))\n",
    "        self.count_unigram = self.count_ngram(unigrams)\n",
    "\n",
    "        bigrams = self.get_ngram(2, train_data)\n",
    "        self.count_bigram = self.count_ngram(bigrams)\n",
    "\n",
    "        trigrams = self.get_ngram(3, train_data)\n",
    "        self.n_trigram = len(trigrams)\n",
    "        self.count_trigram = self.count_ngram(trigrams)\n",
    "\n",
    "        self.prob_trigram = self.prob_with_smoothing()\n",
    "\n",
    "    def test_perplexity(self, test_data):\n",
    "        test_trigrams = self.get_ngram(3, test_data)\n",
    "        test_unigrams = self.get_ngram(1, test_data)\n",
    "        N = len(test_unigrams)\n",
    "        test_joint_prob = 1\n",
    "\n",
    "        for trigram in test_trigrams: \n",
    "            if trigram in self.prob_trigram.keys(): \n",
    "                test_joint_prob *= self.prob_trigram[trigram]\n",
    "            else: \n",
    "                test_joint_prob *= self.prob_trigram[\"<UNK>\"]\n",
    "        \n",
    "        ppl = test_joint_prob ** (-1/N)\n",
    "        return ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551.289217417759"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_model = Trigram() \n",
    "trigram_model.train(train_data)\n",
    "\n",
    "trigram_test_ppl = trigram_model.test_perplexity(test_data)\n",
    "trigram_test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trigrams:  740\n",
      "Number of unique trigrams:  682\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of trigrams: \", trigram_model.n_trigram) \n",
    "print(\"Number of unique trigrams: \", len(trigram_model.count_trigram)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test khác \n",
    "Cùng 1 model, test trên text của Social Studies major ở Fulbright, trend vẫn thế: Unigram tốt nhất, nhì là Trigram, Bigram tồi nhất\n",
    "\n",
    "tại sao vậy :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.2193933509818"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('test2.txt', 'r') as f: \n",
    "    test_social = f.read() \n",
    "\n",
    "mod1 = Unigram() \n",
    "mod1.train(train_data)\n",
    "mod1.test_perplexity(test_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257.12239467771684"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod2 = Bigram() \n",
    "mod2.train(train_data)\n",
    "mod2.test_perplexity(test_social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854.1570338358636"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod3 = Trigram() \n",
    "mod3.train(train_data)\n",
    "mod3.test_perplexity(test_social)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
