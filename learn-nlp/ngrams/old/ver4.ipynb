{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testing_data.txt', 'r') as f:\n",
    "    test_data = f.read()\n",
    "\n",
    "with open('training_data.txt', 'r') as f:\n",
    "    train_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unigram: \n",
    "    def __init__(self):\n",
    "        self.corpus_length = 0 \n",
    "        self.vocab_size = 0 # unique unigrams for everything \n",
    "        self.count = {} \n",
    "        self.prob = {} \n",
    "    \n",
    "    def prob_with_add_one_smoothing(self): \n",
    "        proba = {}\n",
    "        M = self.corpus_length + self.vocab_size\n",
    "        proba[\"<UNK>\"] = 1 / M \n",
    "\n",
    "        for token in self.count.keys(): \n",
    "            proba[token] = (self.count[token] + 1) / M \n",
    "        \n",
    "        return proba\n",
    "\n",
    "    def train(self, train_data):\n",
    "        unigrams = nltk.tokenize.word_tokenize(train_data)\n",
    "        self.count[\"<UNK>\"] = 0\n",
    "        self.corpus_length = len(unigrams)\n",
    "        self.vocab_size = len(set(unigrams)) \n",
    "\n",
    "        for token in unigrams: \n",
    "            if token in self.count.keys():\n",
    "                self.count[token] += 1\n",
    "            else: \n",
    "                self.count[token] = 1\n",
    "\n",
    "        self.prob = self.prob_with_add_one_smoothing()\n",
    "        \n",
    "        return self.prob \n",
    "    \n",
    "    def test_perplexity(self, test_data):\n",
    "        test_tokens = nltk.tokenize.word_tokenize(test_data)\n",
    "        N = len(test_tokens)\n",
    "        ppl = 1 \n",
    "        \n",
    "        for token in test_tokens: \n",
    "            if token in self.prob.keys():\n",
    "                ppl *= 1 / self.prob[token]\n",
    "            else: \n",
    "                ppl *= 1/ self.prob[\"<UNK>\"]\n",
    "        \n",
    "        ppl = ppl ** (1/N)\n",
    "\n",
    "        return ppl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172.1128679443837"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model = Unigram() \n",
    "unigram_model.train(train_data)\n",
    "\n",
    "unigram_test_ppl = unigram_model.test_perplexity(test_data)\n",
    "unigram_test_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bigram: \n",
    "    def __init__(self): \n",
    "        self.corpus_length = 0  # corpus len chinh la number of all bigrams :) change name di \n",
    "        self.vocab_size = 0 \n",
    "        self.n_unique_bigrams = 0 \n",
    "        self.count = {} \n",
    "        self.prob = {} \n",
    "\n",
    "    def prob_with_add_one_smoothing(self):\n",
    "        proba = {}\n",
    "        M = self.corpus_length + self.vocab_size\n",
    "        proba[\"<UNK>\"] = 1 / M \n",
    "\n",
    "        for token in self.count.keys(): # token o day la bigram \n",
    "            joint_prob = self.count(token) / self.count()\n",
    "            proba[token] = (self.count[token] + 1) / M\n",
    "\n",
    "        return proba\n",
    "    \n",
    "    def get_ngram(self, n, text): \n",
    "        unigrams = nltk.tokenize.word_tokenize(text)\n",
    "        ngrams = [] \n",
    "        if n == 1: \n",
    "            return unigrams \n",
    "        else: \n",
    "            last_start = len(unigrams) - n + 1\n",
    "            for i in range(last_start):\n",
    "                ngram = tuple(unigrams[i: i+n])\n",
    "                ngrams.append(ngram)\n",
    "            return unigrams, ngrams\n",
    "    \n",
    "    def train(self, train_data):\n",
    "        unigrams, bigrams = self.get_ngram(2, train_data)\n",
    "        self.vocab_size = len(set(unigrams)) \n",
    "\n",
    "        self.count[\"<UNK>\"] = 0\n",
    "        self.corpus_length = len(bigrams)\n",
    "        self.n_unique_bigrams = len(set(bigrams))\n",
    "\n",
    "        for token in bigrams: \n",
    "            if token in self.count.keys(): \n",
    "                self.count[token] += 1\n",
    "            else: \n",
    "                self.count[token] = 1\n",
    "\n",
    "        self.prob = self.prob_with_add_one_smoothing()\n",
    "        return self.prob \n",
    "    \n",
    "    def test_perplexity(self, test_data):\n",
    "        test_tokens = nltk.tokenize.word_tokenize(test_data)\n",
    "        N = len(test_tokens)\n",
    "        ppl = 1\n",
    "\n",
    "        for token in test_tokens: \n",
    "            if token in self.prob.keys():\n",
    "                ppl *= 1 / self.prob[token]\n",
    "            else: \n",
    "                ppl *= 1/ self.prob[\"<UNK>\"]\n",
    "        \n",
    "        ppl = ppl ** (1/N)\n",
    "\n",
    "        return ppl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1042.9999999999998"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_model = Bigram()\n",
    "bigram_model.train(train_data) \n",
    "\n",
    "bigram_test_ppl = bigram_model.test_perplexity(test_data)\n",
    "bigram_test_ppl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
