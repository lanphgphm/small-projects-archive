{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M7mvge7MOGo"
      },
      "source": [
        "Natural Language Processing - Homework 1 \n",
        "\n",
        "Pham Lan Phuong - 210120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ve1GshEsMPBp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/lanphgphm/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('testing_data.txt', 'r') as f:\n",
        "    test = f.read()\n",
        "\n",
        "with open('training_data.txt', 'r') as f:\n",
        "    train = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ohObtARL4Fc"
      },
      "source": [
        "# Question 1\n",
        "Find all sentences that contain “to be” verbs (i.e. “is”, “are”, ...) in the training data\n",
        "file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['As part of the major, students will be equipped with the foundational knowledge in Computer Science and relevant disciplines.',\n",
              " 'They will be exposed to essential areas of the CS discipline including theory, systems, and applications.',\n",
              " 'They will learn about the underlying mathematical ideas that are critical for computation, establish proficiency in the process of designing systems and applications, gain experience in collecting and analyzing data using modern technologies, and begin to develop an understanding for the role of users in the design of systems and applications.',\n",
              " 'The Computer Science major at Fulbright is designed to prepare students for work in industry or continue their lifelong learning as well as potential graduate-level studies.',\n",
              " 'All students are first required to take the core courses in Liberal Arts and Science.',\n",
              " 'In addition to the two courses in “Global Humanities and Social Change”, and “Modern Vietnamese Culture and Society”, they will be exposed to computational thinking as part of Fulbright’s undergraduate core courses in “Quantitative Reasoning for a Digital Age.”, “Scientific Inquiry”, and “Design and Systems Thinking”.',\n",
              " 'They will be then equipped with the knowledge in the foundational courses in Computer Science including the courses that will lay out the Mathematics Foundation, Software Foundation, and Hardware Foundation, and one course in Professional Responsibilities and Ethics in CS.',\n",
              " 'After having the knowledge in the CS foundation courses, the students will continue their journey with the major courses, which are designed to cover the most important and basic knowledge in the major aspects in Computer Science including a series of six courses that prepare for them to pursue their studies in the concentration areas of Computer Science.']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(train)\n",
        "be_sentences = [] \n",
        "for sentence in sentences:\n",
        "    # use re.search() because we care if the sentence contains 'be' or not\n",
        "    contains_be = re.search(r'\\b(is|are|am|be|being|been|was|were)\\b', sentence)\n",
        "    if contains_be:\n",
        "        be_sentences.append(sentence)\n",
        "\n",
        "be_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGx3FSzpL-81"
      },
      "source": [
        "# Question 2\n",
        "Build a unigram model and a bigram model (both are with add-one smoothing) from the training data file. Then calculate and compare the perplexity score of these two models\n",
        "on the testing data file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Unigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Not using add-one smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_one_smoothing(unigram_count, vocab_size):\n",
        "    for word in unigram_count: \n",
        "        unigram_count[word] += 1 \n",
        "        vocab_size += 1\n",
        "    return unigram_count, vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a dictionary counting word frequency of all words in corpus \n",
        "# create copy of dictionary and add-one smoothing \n",
        "train_words = nltk.tokenize.word_tokenize(train)\n",
        "\n",
        "unigram_count = {}\n",
        "for word in train_words: \n",
        "    if word in unigram_count.keys(): \n",
        "        unigram_count[word] += 1\n",
        "    else: \n",
        "        unigram_count[word] = 1\n",
        "vocab_size = len(unigram_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vocab_size is the number of objects in unigram_count \n",
        "unigram_probability = {}\n",
        "\n",
        "for word in unigram_count.keys():\n",
        "    unigram_probability[word] = unigram_count[word] / vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute perplexity of the entire test corpus \n",
        "# using unigram model without smoothing\n",
        "def unigram_ppl(test, unigram_probability):\n",
        "    test_words = nltk.tokenize.word_tokenize(test)\n",
        "    test_ppl = 1 \n",
        "\n",
        "    for word in test_words: \n",
        "        if word in unigram_probability.keys():\n",
        "            test_ppl *= 1/unigram_probability[word]\n",
        "\n",
        "    test_ppl = test_ppl ** (1/len(test_words))\n",
        "    return test_ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During testing, the model may see a word that it has not seen in training. \n",
        "To tackle this, I saw someone suggested creating a separate token for the\n",
        "unseen word called <UNKNOWN>, and assign P(<UNKNOWN>) = 1 / vocab_size. \n",
        "This makes sense, as the larger the vocab size, the less likely that the \n",
        "model will see a new word in testing. \n",
        "\n",
        "However, chapter 4 of the textbook says that it's okay for now to ignore \n",
        "unseen words, so that's what I did for the unigram_ppl() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25.81394472305185"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigram_ppl(test, unigram_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using add-one smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "unigram_count_copy = deepcopy(unigram_count)\n",
        "vocab_size_copy = deepcopy(vocab_size)\n",
        "\n",
        "smoothed_unigram_count, smoothed_vocab_size = add_one_smoothing(unigram_count_copy, vocab_size_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "smoothed_unigram_probability = {}\n",
        "\n",
        "for word in smoothed_unigram_count.keys():\n",
        "    smoothed_unigram_probability[word] = smoothed_unigram_count[word] / smoothed_vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "fH0G46upMHvV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37.57928459350777"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compute perplexity of the entire test corpus \n",
        "# using unigram model with add-one smoothing\n",
        "unigram_ppl(test, smoothed_unigram_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using add-one smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "zDlHdDLRLpcz"
      },
      "outputs": [],
      "source": [
        "# create a raw bigram count\n",
        "bigrams = list(ngrams(sequence=train_words, n=2))\n",
        "bigram_count = {}\n",
        "\n",
        "for pair in bigrams: \n",
        "    if pair in bigram_count.keys(): \n",
        "        bigram_count[pair] += 1\n",
        "    else: \n",
        "        bigram_count[pair] = 1\n",
        "\n",
        "bigram_size = len(bigram_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "bigram_count_copy = deepcopy(bigram_count)\n",
        "bigram_size_copy = deepcopy(bigram_size)\n",
        "\n",
        "smoothed_bigram_count, smoothed_bigram_size = \\\n",
        "    add_one_smoothing(bigram_count_copy, bigram_size_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "smoothed_bigram_probability = {}\n",
        "\n",
        "for pair in smoothed_bigram_count.keys():\n",
        "    smoothed_bigram_probability[pair] = smoothed_bigram_count[pair] / smoothed_bigram_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute perplexity for the entire test corpus \n",
        "def bigram_ppl(test, smoothed_bigram_probability, smoothed_unigram_probability):\n",
        "    test_words = nltk.tokenize.word_tokenize(test) \n",
        "    test_bigrams = list(ngrams(sequence=test_words, n=2))\n",
        "    test_ppl = 1\n",
        "\n",
        "    for pair in test_bigrams: \n",
        "        context = pair[0]\n",
        "        if pair in smoothed_bigram_probability.keys():\n",
        "            cond_prob = smoothed_bigram_probability[pair] / smoothed_unigram_probability[context]\n",
        "            test_ppl *= 1/cond_prob\n",
        "            \n",
        "    \n",
        "    test_ppl = test_ppl ** (1/len(test_words))\n",
        "    return test_ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.0643605162383167"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_ppl(test, smoothed_bigram_probability, smoothed_unigram_probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.485475943355908"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bigram_ppl(test, smoothed_bigram_probability, unigram_probability)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
