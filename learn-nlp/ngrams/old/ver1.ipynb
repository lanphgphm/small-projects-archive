{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPVIqIaTZ602"
      },
      "source": [
        "Natural Language Processing - Homework 1\n",
        "\n",
        "Pham Lan Phuong - 210120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaX875MwZ607",
        "outputId": "9b6f7ce2-658d-4fac-ca18-10c5d99685f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/lanphgphm/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnY9YogmZ60-"
      },
      "outputs": [],
      "source": [
        "with open('testing_data.txt', 'r') as f:\n",
        "    test = f.read()\n",
        "\n",
        "with open('training_data.txt', 'r') as f:\n",
        "    train = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_yGV-rkZ60_"
      },
      "source": [
        "# Question 1\n",
        "Find all sentences that contain “to be” verbs (i.e. “is”, “are”, ...) in the training data\n",
        "file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn1TLe_KZ61A",
        "outputId": "8794016e-7764-41b9-ffd1-92ea34eb2996"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['As part of the major, students will be equipped with the foundational knowledge in Computer Science and relevant disciplines.',\n",
              " 'They will be exposed to essential areas of the CS discipline including theory, systems, and applications.',\n",
              " 'They will learn about the underlying mathematical ideas that are critical for computation, establish proficiency in the process of designing systems and applications, gain experience in collecting and analyzing data using modern technologies, and begin to develop an understanding for the role of users in the design of systems and applications.',\n",
              " 'The Computer Science major at Fulbright is designed to prepare students for work in industry or continue their lifelong learning as well as potential graduate-level studies.',\n",
              " 'All students are first required to take the core courses in Liberal Arts and Science.',\n",
              " 'In addition to the two courses in “Global Humanities and Social Change”, and “Modern Vietnamese Culture and Society”, they will be exposed to computational thinking as part of Fulbright’s undergraduate core courses in “Quantitative Reasoning for a Digital Age.”, “Scientific Inquiry”, and “Design and Systems Thinking”.',\n",
              " 'They will be then equipped with the knowledge in the foundational courses in Computer Science including the courses that will lay out the Mathematics Foundation, Software Foundation, and Hardware Foundation, and one course in Professional Responsibilities and Ethics in CS.',\n",
              " 'After having the knowledge in the CS foundation courses, the students will continue their journey with the major courses, which are designed to cover the most important and basic knowledge in the major aspects in Computer Science including a series of six courses that prepare for them to pursue their studies in the concentration areas of Computer Science.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = nltk.tokenize.sent_tokenize(train)\n",
        "be_sentences = []\n",
        "for sentence in sentences:\n",
        "    # use re.search() because we care if the sentence contains 'be' or not\n",
        "    contains_be = re.search(r'\\b(is|are|am|be|being|been|was|were)\\b', sentence)\n",
        "    if contains_be:\n",
        "        be_sentences.append(sentence)\n",
        "\n",
        "be_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA0YTTCNZ61C"
      },
      "source": [
        "# Question 2\n",
        "Build a unigram model and a bigram model (both are with add-one smoothing) from the training data file. Then calculate and compare the perplexity score of these two models\n",
        "on the testing data file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luWtTYcEZ61C"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab4JRByAZ61D"
      },
      "outputs": [],
      "source": [
        "def add_one_smoothing(ngram_count, unique_ngram_size):\n",
        "    for word in ngram_count:\n",
        "        ngram_count[word] += 1\n",
        "        unique_ngram_size += 1\n",
        "    return ngram_count, unique_ngram_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaMOI4EIZ61E"
      },
      "outputs": [],
      "source": [
        "def ngram_probability(ngram_count, unique_ngram_size):\n",
        "    for word in ngram_count:\n",
        "        ngram_count[word] = ngram_count[word] / unique_ngram_size\n",
        "    return ngram_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCvet1gAZ61G"
      },
      "outputs": [],
      "source": [
        "def unigram_ppl(test, unigram_probability):\n",
        "    test_words = nltk.tokenize.word_tokenize(test)\n",
        "    test_ppl = 1\n",
        "\n",
        "    for word in test_words:\n",
        "        if word in unigram_probability.keys():\n",
        "            test_ppl *= 1/unigram_probability[word]\n",
        "\n",
        "    test_ppl = test_ppl ** (1/len(test_words))\n",
        "    return test_ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMnsYt5jZ61H"
      },
      "outputs": [],
      "source": [
        "def bigram_ppl(test, bigram_probability, unigram_probability):\n",
        "    test_words = nltk.tokenize.word_tokenize(test)\n",
        "    test_bigrams = list(ngrams(sequence=test_words, n=2))\n",
        "    test_ppl = 1\n",
        "\n",
        "    for pair in test_bigrams:\n",
        "        context = pair[0]\n",
        "        if pair in bigram_probability.keys():\n",
        "            cond_prob = bigram_probability[pair] / unigram_probability[context]\n",
        "            test_ppl *= 1/cond_prob\n",
        "\n",
        "\n",
        "    test_ppl = test_ppl ** (1/len(test_words))\n",
        "    return test_ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpgDvhWIZ61J"
      },
      "source": [
        "### Unigram & add-one smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofUtRqcBZ61K"
      },
      "outputs": [],
      "source": [
        "# create a dictionary counting word frequency of all words in corpus\n",
        "# create copy of dictionary and add-one smoothing\n",
        "train_words = nltk.tokenize.word_tokenize(train)\n",
        "\n",
        "unigram_count = {}\n",
        "for word in train_words:\n",
        "    if word in unigram_count.keys():\n",
        "        unigram_count[word] += 1\n",
        "    else:\n",
        "        unigram_count[word] = 1\n",
        "\n",
        "unigram_size = len(unigram_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAayiwHCZ61K"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "unigram_count_copy = deepcopy(unigram_count)\n",
        "unigram_size_copy = deepcopy(unigram_size)\n",
        "\n",
        "smoothed_unigram_count, smoothed_vocab_size = \\\n",
        "    add_one_smoothing(unigram_count_copy, unigram_size_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_yDSwjxZ61L",
        "outputId": "be822f58-bb18-4426-d881-c7480764ed18"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37.57928459350777"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_unigram_probability = ngram_probability(smoothed_unigram_count, smoothed_vocab_size)\n",
        "unigram_ppl(test, smoothed_unigram_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPUMPwfiZ61M"
      },
      "source": [
        "### Bigram & add-one smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXUHQ6pKZ61M"
      },
      "outputs": [],
      "source": [
        "# create a raw bigram count\n",
        "bigrams = list(ngrams(sequence=train_words, n=2))\n",
        "bigram_count = {}\n",
        "\n",
        "for pair in bigrams:\n",
        "    if pair in bigram_count.keys():\n",
        "        bigram_count[pair] += 1\n",
        "    else:\n",
        "        bigram_count[pair] = 1\n",
        "\n",
        "bigram_size = len(bigram_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSWMrvLKZ61N"
      },
      "outputs": [],
      "source": [
        "bigram_count_copy = deepcopy(bigram_count)\n",
        "bigram_size_copy = deepcopy(bigram_size)\n",
        "\n",
        "smoothed_bigram_count, smoothed_bigram_size = \\\n",
        "    add_one_smoothing(bigram_count_copy, bigram_size_copy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKcAetnJZ61O",
        "outputId": "9f5c86b4-c749-4298-cbfe-ddfa5be0a65f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.0643605162383167"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "smoothed_bigram_probability = ngram_probability(smoothed_bigram_count, smoothed_bigram_size)\n",
        "bigram_ppl(test, smoothed_bigram_probability, smoothed_unigram_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the same training corpus, bigram has significantly lower perplexity score of\n",
        "2.0643605162383167 compared to unigram with perplexity score of 37.57928459350777.\n",
        "\n",
        "Bigram performs better on test set."
      ],
      "metadata": {
        "id": "8PDh_lTrbJRz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zRR8RFLZ61P"
      },
      "source": [
        "### Intermediate results for Question 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JfFrCkW7Z61P",
        "outputId": "0162afc4-95b4-4b91-8658-81965c048200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(',', 52),\n",
              " ('and', 37),\n",
              " ('.', 31),\n",
              " ('the', 30),\n",
              " ('in', 27),\n",
              " ('of', 22),\n",
              " ('to', 20),\n",
              " ('courses', 14),\n",
              " ('will', 12),\n",
              " ('Science', 10),\n",
              " ('their', 10),\n",
              " ('Computer', 9),\n",
              " ('students', 9),\n",
              " ('for', 9),\n",
              " ('major', 8),\n",
              " ('a', 7),\n",
              " ('science', 7),\n",
              " ('with', 6),\n",
              " ('knowledge', 6),\n",
              " ('an', 5)]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "key = lambda x: x[1]\n",
        "sorted_unigram_count = sorted(unigram_count.items(), key=key, reverse=True)\n",
        "sorted_unigram_count[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40mHcIHvZ61Q",
        "outputId": "f2996e0f-30b9-484a-db7b-3abe2451c6ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[((',', 'and'), 12),\n",
              " (('Computer', 'Science'), 9),\n",
              " (('in', 'the'), 8),\n",
              " (('Science', 'major'), 4),\n",
              " (('will', 'be'), 4),\n",
              " (('knowledge', 'in'), 4),\n",
              " (('in', 'Computer'), 4),\n",
              " (('areas', 'of'), 4),\n",
              " (('and', 'applications'), 4),\n",
              " (('science', ','), 4),\n",
              " (('computer', 'science'), 4),\n",
              " (('courses', 'in'), 4),\n",
              " (('”', ','), 4),\n",
              " (('of', 'the'), 3),\n",
              " (('the', 'major'), 3),\n",
              " (('students', 'will'), 3),\n",
              " (('with', 'the'), 3),\n",
              " (('.', 'They'), 3),\n",
              " (('They', 'will'), 3),\n",
              " (('applications', '.'), 3)]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted_bigram_count = sorted(bigram_count.items(), key=key, reverse=True)\n",
        "sorted_bigram_count[:20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxtd6Hl4Z61R",
        "outputId": "15f4b9b9-4ed1-4147-cfc9-fbdabee7baf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P(and|,) =  0.12533920761102066\n",
            "P(Science|Computer) =  0.5109983079526226\n",
            "P(the|in) =  0.16424945612762873\n",
            "P(major|Science) =  0.23227195816028304\n",
            "P(be|will) =  0.19653781075100873\n",
            "P(in|knowledge) =  0.36499879139473046\n",
            "P(Computer|in) =  0.09124969784868261\n",
            "P(of|areas) =  0.4258319232938522\n",
            "P(applications|and) =  0.06723661946745035\n",
            "P(,|science) =  0.3193739424703892\n",
            "P(science|computer) =  0.5109983079526226\n",
            "P(in|courses) =  0.1703327693175409\n",
            "P(,|”) =  0.4258319232938522\n",
            "P(the|of) =  0.0888692709482822\n",
            "P(major|the) =  0.0659352655422739\n",
            "P(will|students) =  0.20439932318104906\n",
            "P(the|with) =  0.29199903311578435\n",
            "P(They|.) =  0.06387478849407784\n",
            "P(will|They) =  0.5109983079526227\n",
            "P(.|applications) =  0.4087986463620981\n"
          ]
        }
      ],
      "source": [
        "# showing bigram conditional probability,\n",
        "# code taken from function bigram_ppl() above\n",
        "# showing top 20 bigrams sorted by joint probability, not showing all because too long\n",
        "top_20_bigrams = dict(sorted_bigram_count[:20])\n",
        "\n",
        "for pair in top_20_bigrams.keys():\n",
        "    context = pair[0]\n",
        "    if pair in smoothed_bigram_probability.keys():\n",
        "        cond_prob = smoothed_bigram_probability[pair] / smoothed_unigram_probability[context]\n",
        "        print(f'P({pair[1]}|{pair[0]}) = ', cond_prob)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}