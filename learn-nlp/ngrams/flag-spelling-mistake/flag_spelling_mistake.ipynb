{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Y2nCbGGpAM"
      },
      "source": [
        "# Unigram & Bigram Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCaN4m6tGpAQ",
        "outputId": "587a58ee-c5fe-4517-a241-dba6e7103424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg2037M5GpAS"
      },
      "source": [
        "## Unigram\n",
        "To flag words the model has never seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IYGo5sFWGpAT"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class Unigram: for unigram language model.\n",
        "\n",
        "Attributes:\n",
        "- n_unigrams: number of tokens in training data\n",
        "- vocab_size: number of unique unigram in training data\n",
        "- count: a dictionary of unigram counts\n",
        "\n",
        "Methods:\n",
        "- train(train_data): train the unigram model\n",
        "- compute_prob(unigram): compute the probability of a unigram using\n",
        "                        unigram count and add-one smoothing\n",
        "- test_perplexity(test_data): compute the perplexity of test data using\n",
        "                        log likelihood method to avoid underflow\n",
        "'''\n",
        "\n",
        "class Unigram:\n",
        "    def __init__(self):\n",
        "        self.n_unigram = 0\n",
        "        self.vocab_size = 0\n",
        "        self.count = {}\n",
        "\n",
        "    def train(self, train_data):\n",
        "        '''\n",
        "        This function trains the unigram model. After running this funtion,\n",
        "        a dictionary storing counts of each unigram will be created.\n",
        "\n",
        "        Input:\n",
        "            train_data: string\n",
        "                training data to train the unigram model\n",
        "        Return:\n",
        "            None\n",
        "        '''\n",
        "        # tokenize & assigning values to model attributes\n",
        "        unigrams = nltk.tokenize.word_tokenize(train_data)\n",
        "        # self.count[\"<UNK>\"] = 0 -- REMOVE UNK TOKEN FOR FLAGGING TYPO\n",
        "        self.n_unigram = len(unigrams)\n",
        "        self.vocab_size = len(set(unigrams))\n",
        "\n",
        "        # creating unigram count dictionary\n",
        "        for unigram in unigrams:\n",
        "            if unigram in self.count.keys():\n",
        "                self.count[unigram] += 1\n",
        "            else:\n",
        "                self.count[unigram] = 1\n",
        "\n",
        "\n",
        "    def compute_prob(self, unigram):\n",
        "        '''\n",
        "        This function takes in a unigram, and returns the probability that\n",
        "        unigram appears in the training data with add-one smoothing.\n",
        "\n",
        "        Input:\n",
        "            unigram: string\n",
        "                the unigram to compute probability for\n",
        "        Return:\n",
        "            the probability of the given unigram appearing in the training data\n",
        "        '''\n",
        "        N = self.n_unigram\n",
        "        V = self.vocab_size\n",
        "        if (unigram in self.count.keys()):\n",
        "            return (self.count[unigram] + 1) / (N + V) # smoothing\n",
        "        else:\n",
        "            return 1 / (N + V) # smoothing for unseen words\n",
        "\n",
        "    def test_perplexity(self, test_data):\n",
        "        '''\n",
        "        This function takes in a test data, and returns the perplexity\n",
        "        of this data on the unigram model. The perplexity is computed\n",
        "        using log likelihood method to avoid underflow.\n",
        "\n",
        "        Input:\n",
        "            test_data: string\n",
        "                the test data to compute perplexity for\n",
        "        Return:\n",
        "            the perplexity of the test data on the unigram model\n",
        "        '''\n",
        "        test_unigrams = nltk.tokenize.word_tokenize(test_data)\n",
        "        M = len(test_unigrams)\n",
        "\n",
        "        # perplexity = exponential of negative average log likelihood\n",
        "        probs = []\n",
        "        for unigram in test_unigrams:\n",
        "            probs.append(self.compute_prob(unigram))\n",
        "\n",
        "        avg_log_likelihood = np.log(probs).sum() / M\n",
        "        ppl = np.exp((-1) * avg_log_likelihood)\n",
        "        return ppl\n",
        "\n",
        "    def flag_unseen(self, test_data):\n",
        "        '''\n",
        "        This function takes in a sentence, and returns the lists of words\n",
        "        that might be a typo. A word is considered a typo if it is not in\n",
        "        the training data.\n",
        "\n",
        "        Input:\n",
        "            test_data: string\n",
        "                the test data to flag unseen words for\n",
        "        Return:\n",
        "            unseen: list of strings\n",
        "                the list of words that might be a typo\n",
        "        '''\n",
        "        test_unigrams = nltk.tokenize.word_tokenize(test_data)\n",
        "        unseen = []\n",
        "        for unigram in test_unigrams:\n",
        "            if unigram not in self.count.keys():\n",
        "                unseen.append(unigram)\n",
        "        return unseen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grRYMR6NGpAU"
      },
      "source": [
        "## Bigram\n",
        "To flag pairs of words that seems to not often go together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qvp0epMLGpAV"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Class Bigram: for bigram language model.\n",
        "\n",
        "Attributes:\n",
        "- count_unigram: a dictionary of unigram counts\n",
        "- count_bigram: a dictionary of bigram counts\n",
        "- n_unigrams: number of unigram in training data\n",
        "- n_bigrams: number of bigram tokens in training data\n",
        "- vocab_size: number of unique unigram in training data\n",
        "\n",
        "\n",
        "Methods:\n",
        "- get_ngram(n, text): get ngrams from text\n",
        "- count_ngram(ngrams): count ngrams frequency\n",
        "- train(train_data): train the bigram model\n",
        "- compute_prob(bigram): compute the probability of a bigram appearing\n",
        "                        using add-one smoothing\n",
        "- test_perplexity(test_data): compute the perplexity of test data using\n",
        "                        log likelihood method to avoid underflow\n",
        "'''\n",
        "\n",
        "class Bigram:\n",
        "    def __init__(self):\n",
        "        self.count_unigram = {}\n",
        "        self.count_bigram = {}\n",
        "        self.n_unigram = 0\n",
        "        self.n_bigram = 0\n",
        "\n",
        "        self.vocab_size = 0\n",
        "\n",
        "\n",
        "    def get_ngram(self, n, text):\n",
        "        '''\n",
        "        This function takes in a text and returns a list of ngrams.\n",
        "        If n = 1, return a list of unigrams using nltk.tokenize module.\n",
        "        Higher ngrams are manually created with sliding window.\n",
        "\n",
        "        Input:\n",
        "            n: int\n",
        "                the n in 'ngram'\n",
        "            text: string\n",
        "                the text to be split into ngrams\n",
        "\n",
        "        Return:\n",
        "            a list of ngrams\n",
        "        '''\n",
        "        unigrams = nltk.tokenize.word_tokenize(text)\n",
        "        ngrams = []\n",
        "        if n == 1:\n",
        "            return unigrams\n",
        "        else:\n",
        "            last_start = len(unigrams) - n + 1\n",
        "            for i in range(last_start):\n",
        "                ngram = tuple(unigrams[i: i+n])\n",
        "                ngrams.append(ngram)\n",
        "            return ngrams\n",
        "\n",
        "    def count_ngram(self, ngrams):\n",
        "        '''\n",
        "        This function takes in a list of ngrams and returns a dictionary\n",
        "        counting the frequency of each ngram.\n",
        "\n",
        "        Input:\n",
        "            ngrams: list\n",
        "                a list of ngrams\n",
        "\n",
        "        Return:\n",
        "            a dictionary of ngram counts\n",
        "        '''\n",
        "        count = {}\n",
        "        # count[\"<UNK>\"] = 0\n",
        "\n",
        "        for ngram in ngrams:\n",
        "            if ngram in count.keys():\n",
        "                count[ngram] += 1\n",
        "            else:\n",
        "                count[ngram] = 1\n",
        "        return count\n",
        "\n",
        "    def train(self, train_data):\n",
        "        '''\n",
        "        This function trains the bigram model. After running this funtion,\n",
        "        two dictionaries storing counts of each unigram and bigram will be\n",
        "        created.\n",
        "\n",
        "        Input:\n",
        "            train_data: string\n",
        "                training data to train the bigram model\n",
        "        Return:\n",
        "            None\n",
        "        '''\n",
        "        # tokenize unigrams & assigning unigram attributes\n",
        "        unigrams = self.get_ngram(1, train_data)\n",
        "        self.n_unigram = len(unigrams)\n",
        "        self.vocab_size = len(set(unigrams))\n",
        "        self.count_unigram = self.count_ngram(unigrams)\n",
        "\n",
        "        # tokenize bigrams & assigning bigram attributes\n",
        "        bigrams = self.get_ngram(2, train_data)\n",
        "        self.n_bigram = len(bigrams)\n",
        "        self.count_bigram = self.count_ngram(bigrams)\n",
        "\n",
        "    def compute_prob(self, bigram):\n",
        "        '''\n",
        "        This function takes in a bigram, and returns the probability that\n",
        "        bigram appears in the training data with add-one smoothing.\n",
        "\n",
        "        In case of unseen bigrams (a, b):\n",
        "        (1) a is unseen, b is seen:\n",
        "            P(a, b) = (count(a, b) +  1 )/ (count(a) + V)\n",
        "                    = 1 / V\n",
        "        (2) a is seen, b is unseen:\n",
        "            P(a, b) = count(a, b) + 1 / count(a) + V\n",
        "                    = 1 / count(a) + V\n",
        "        (3) a is unseen, b is unseen:\n",
        "            P(a, b) = count(a, b) + 1 / count(a) + V\n",
        "                    = 1 / V\n",
        "        (4) a is seen, b is seen, but in the wrong order:\n",
        "            P(a, b) = count(a, b) + 1 / count(a) + V\n",
        "                    = 1 / count(a) + V\n",
        "\n",
        "        Input:\n",
        "            bigram: tuple\n",
        "                the bigram to compute probability for\n",
        "        Return:\n",
        "            the probability of the given bigram appearing in the training data\n",
        "        '''\n",
        "        ctx = bigram[0]\n",
        "        if (ctx in self.count_unigram.keys()):\n",
        "            context = self.count_unigram[ctx]\n",
        "        else:\n",
        "            context = 0\n",
        "\n",
        "        if (bigram in self.count_bigram.keys()):\n",
        "            joint = self.count_bigram[bigram]\n",
        "        else:\n",
        "            joint = 0\n",
        "\n",
        "        return (joint + 1) / (context + self.vocab_size) # smoothing\n",
        "\n",
        "    def test_perplexity(self, test_data):\n",
        "        '''\n",
        "        This function takes in a test data, and returns the perplexity\n",
        "        of this data on the bigram model. The perplexity is computed\n",
        "        using log likelihood method to avoid underflow.\n",
        "\n",
        "        Input:\n",
        "            test_data: string\n",
        "                the test data to compute perplexity for\n",
        "        Return:\n",
        "            the perplexity of the test data on the bigram model\n",
        "        '''\n",
        "        test_bigrams = self.get_ngram(2, test_data)\n",
        "        test_unigrams = self.get_ngram(1, test_data)\n",
        "        M = len(test_unigrams)\n",
        "\n",
        "        probs = []\n",
        "\n",
        "        # compute the probability that the first word in test data appears\n",
        "        # P(first) is the probaility that this unigram appears in training data\n",
        "        first_word = test_unigrams[0]\n",
        "        p_first_word = 1\n",
        "        if (first_word in self.count_unigram.keys()):\n",
        "            # if first_word is seen\n",
        "            p_first_word = (self.count_unigram[first_word] + 1) \\\n",
        "                  / (self.n_unigram + self.vocab_size)\n",
        "        else:\n",
        "            # first_word is unseen\n",
        "            p_first_word = 1 / (self.n_unigram + self.vocab_size)\n",
        "\n",
        "        probs.append(p_first_word)\n",
        "\n",
        "        for bigram in test_bigrams:\n",
        "            probs.append(self.compute_prob(bigram))\n",
        "\n",
        "        avg_log_likelihood = np.log(probs).sum() / M\n",
        "        ppl = np.exp((-1) * avg_log_likelihood)\n",
        "        return ppl\n",
        "\n",
        "    def flag_typo(self, test_data, delta=0.00001):\n",
        "        '''\n",
        "        This function takes in a sentence, and returns the list of words\n",
        "        that might be a typo (spelling mistake) in the sentence.\n",
        "\n",
        "        A pair of word (bigram) is considered a typo if it is\n",
        "        has a probability of occurring less than delta in the\n",
        "        training data.\n",
        "\n",
        "        *Note: the default delta is arbitrary and can be adjusted.\n",
        "\n",
        "        Input:\n",
        "            test_data: string\n",
        "                the test data to flag typo for\n",
        "        Return:\n",
        "            typo: list of strings\n",
        "                the list of words that might be a typo\n",
        "        '''\n",
        "        test_bigrams = self.get_ngram(2, test_data)\n",
        "        typo = []\n",
        "        for bigram in test_bigrams:\n",
        "            prob = self.compute_prob(bigram)\n",
        "            if prob < delta:\n",
        "                typo.append(bigram[1])\n",
        "        return typo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilhBjAkRGpAV"
      },
      "source": [
        "# Spelling Mistake Checking"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxh7xzHHG2pP",
        "outputId": "d7fba13b-3382-44e2-c54d-28b15028db72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NI3SYPcRGpAW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "wiki = load_dataset(\"viettelai/wiki-dump-cleaned\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "tIwViwWyGpAW"
      },
      "outputs": [],
      "source": [
        "# write wiki[\"text\"] to a single txt file\n",
        "i = 0\n",
        "SHRINK_FACTOR = 100\n",
        "\n",
        "with open(\"small_wiki.txt\", \"w\") as f:\n",
        "    for text in wiki[\"text\"]:\n",
        "        if (i%SHRINK_FACTOR==0):\n",
        "            f.write(text + \"\\n\")\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Ezx-xv1nGpAX"
      },
      "outputs": [],
      "source": [
        "with open(\"small_wiki.txt\", \"r\") as f:\n",
        "    data = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lyTvq29UGpAY"
      },
      "outputs": [],
      "source": [
        "unigram = Unigram()\n",
        "unigram.train(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Gy_IJWqvGpAZ"
      },
      "outputs": [],
      "source": [
        "bigram = Bigram()\n",
        "bigram.train(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "M_5eI1hZGpAa"
      },
      "outputs": [],
      "source": [
        "def find_spelling_mistakes(text, pretrained_unigram, pretrained_bigram, delta=0.000001):\n",
        "    print(\"Câu gốc: \", text)\n",
        "    print(\"Những từ có thể bị sai chính tả:\")\n",
        "    print(pretrained_unigram.flag_unseen(text))\n",
        "\n",
        "    print(\"Những từ lạ:\")\n",
        "    print(pretrained_bigram.flag_typo(text, delta))\n",
        "\n",
        "    print(\"Perplexity của câu: \", pretrained_bigram.test_perplexity(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LNRa9TTDGpAb"
      },
      "outputs": [],
      "source": [
        "cau_oke = [\n",
        "    \"Trời đang mưa, nhưng tôi vẫn cảm thấy vui vẻ.\",\n",
        "    \"Con đường dài, nhưng đích đến luôn gần gũi.\",\n",
        "    \"Ngày nay, tôi đã học được nhiều điều mới.\",\n",
        "    \"Sự sáng tạo không bao giờ ngừng lại.\",\n",
        "    \"Cuộc sống không phải lúc nào cũng dễ dàng, nhưng chúng ta vẫn cố gắng.\",\n",
        "    \"Tôi muốn đi du lịch, nhưng tôi không có thời gian.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-wscEg6iGpAb"
      },
      "outputs": [],
      "source": [
        "cau_loi = [\n",
        "    \"Tại sao tao không thấy mày đứng diowis sân trường?\",\n",
        "    \"Một điều tôi thích ở Thái Lan là thực ẩm đường phố của họ.\",\n",
        "    \"Tôi thích ở nhà vì tôi có thể nấu ăn và việc làm nhà.\",\n",
        "    \"Chai nước này đang thoại điện thoại của tôi.\",\n",
        "    \"Họ gọi em là thg điên rồi cúp máy luôn.\",\n",
        "    \"Ăn cơm với cái gì cx đc, ko ăn cx ksao hết.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLkli8Q2GpAb",
        "outputId": "2e1f7bab-0d0e-490b-c955-95381825bc16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu gốc:  Trời đang mưa, nhưng tôi vẫn cảm thấy vui vẻ.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  9656.596333540645\n",
            "Câu gốc:  Con đường dài, nhưng đích đến luôn gần gũi.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  9776.668923410354\n",
            "Câu gốc:  Ngày nay, tôi đã học được nhiều điều mới.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  2205.8036100212967\n",
            "Câu gốc:  Sự sáng tạo không bao giờ ngừng lại.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  4433.043928903044\n",
            "Câu gốc:  Cuộc sống không phải lúc nào cũng dễ dàng, nhưng chúng ta vẫn cố gắng.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  2938.2384628019713\n",
            "Câu gốc:  Tôi muốn đi du lịch, nhưng tôi không có thời gian.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  2041.3760648964585\n"
          ]
        }
      ],
      "source": [
        "for cau in cau_oke:\n",
        "    find_spelling_mistakes(cau, unigram, bigram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h30D3FKbGpAc",
        "outputId": "c87e581c-9cec-41a6-b258-79392c9e06dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu gốc:  Tại sao tao không thấy mày đứng diowis sân trường?\n",
            "Những từ có thể bị sai chính tả:\n",
            "['diowis']\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  39834.49600825416\n",
            "Câu gốc:  Một điều tôi thích ở Thái Lan là thực ẩm đường phố của họ.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  4577.326628882519\n",
            "Câu gốc:  Tôi thích ở nhà vì tôi có thể nấu ăn và việc làm nhà.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  5995.664338959581\n",
            "Câu gốc:  Chai nước này đang thoại điện thoại của tôi.\n",
            "Những từ có thể bị sai chính tả:\n",
            "[]\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  10164.901389644388\n",
            "Câu gốc:  Họ gọi em là thg điên rồi cúp máy luôn.\n",
            "Những từ có thể bị sai chính tả:\n",
            "['thg']\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  39147.75991511004\n",
            "Câu gốc:  Ăn cơm với cái gì cx đc, ko ăn cx ksao hết.\n",
            "Những từ có thể bị sai chính tả:\n",
            "['cx', 'đc', 'cx', 'ksao']\n",
            "Những từ lạ:\n",
            "[]\n",
            "Perplexity của câu:  45757.33185487031\n"
          ]
        }
      ],
      "source": [
        "for cau in cau_loi:\n",
        "    find_spelling_mistakes(cau, unigram, bigram)\n",
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}